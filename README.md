# Benchmark de Performances â€“ Web Services REST

Profil GitHub : [https://github.com/ABDELLAH1234512345](https://github.com/ABDELLAH1234512345)

Ce projet propose une plateforme complÃ¨te pour comparer les performances de trois variantes dâ€™implÃ©mentation de services REST en Java, Ã  lâ€™aide de scÃ©narios JMeter et dâ€™un monitoring avancÃ© (Prometheus, Grafana, PostgreSQL via Docker).

## ğŸš€ Objectifs
- **Comparer** diffÃ©rentes stacks Java pour des API REST (Jersey+Hibernate, Spring MVC, Spring Data REST)
- **Mesurer** la performance sous charge (latence, RPS, erreurs, etc.)
- **Visualiser** les rÃ©sultats via Grafana
- **Automatiser** les tests et la collecte de mÃ©triques

## ğŸ—ï¸ Structure du projet

```
Benchmark-de-performances-main/
â”œâ”€â”€ docker-compose.yml         # Stack monitoring (Postgres, Prometheus, Grafana)
â”œâ”€â”€ pom.xml                    # DÃ©pendances Maven globales
â”œâ”€â”€ run_all_benchmarks.sh      # ExÃ©cution de tous les benchmarks
â”œâ”€â”€ jmeter/                    # ScÃ©narios et payloads JMeter
â”œâ”€â”€ monitoring/                # Config Prometheus & Grafana
â”œâ”€â”€ src/main/java/com/example/ # ImplÃ©mentation Spring (variant de base)
â”œâ”€â”€ variant1/                  # Variante A: Jersey + Hibernate
â”œâ”€â”€ variant2/                  # Variante C: Spring MVC + Hibernate
â”œâ”€â”€ variant3/                  # Variante D: Spring Data REST
```

## ğŸ§© Variantes comparÃ©es
- **Variante A** : Jersey (JAX-RS) + Hibernate ([variant1/](variant1/))
- **Variante C** : Spring MVC + Hibernate ([variant2/](variant2/))
- **Variante D** : Spring Data REST ([variant3/](variant3/))

Chaque variante expose des endpoints REST pour gÃ©rer des entitÃ©s `Item` et `Category`.

## âš¡ DÃ©marrage rapide

### 1. PrÃ©requis
- Java 17+
- Maven 3.6+
- Docker & Docker Compose
- JMeter (pour les tests de charge)

### 2. Lancer lâ€™infrastructure de monitoring
```bash
docker-compose up -d
```
- AccÃ¨s Grafana : http://localhost:3003 (admin/admin)
- AccÃ¨s Prometheus : http://localhost:9094

### 3. DÃ©marrer une variante
Exemple pour la variante AÂ :
```bash
cd variant1
./run.sh
```
Pour les autres variantes, adaptez le dossier (`variant2`, `variant3`) et le port (8081, 8082).

### 4. GÃ©nÃ©rer des donnÃ©es de test
Chaque variante propose un service de gÃ©nÃ©ration de donnÃ©es (catÃ©gories, items) pour simuler une base rÃ©aliste (voir scripts ou endpoints spÃ©cifiques).

### 5. Lancer les benchmarks JMeter
Depuis chaque dossier de varianteÂ :
```bash
./jmeter_tests.sh
```
Les rÃ©sultats HTML sont gÃ©nÃ©rÃ©s dans `jmeter-results/`.

### 6. Comparer les rÃ©sultats
- Les dashboards Grafana permettent de comparer RPS, latence, erreurs, etc. entre variantes.
- Les rapports JMeter HTML dÃ©taillent chaque scÃ©nario.

## ğŸ“Š ScÃ©narios de test JMeter
- **READ-heavy** : lectures massives, pagination, relations
- **JOIN-filter** : requÃªtes avec jointures et filtres
- **MIXED** : Ã©critures et lectures simultanÃ©es
- **HEAVY-body** : payloads volumineux

Voir [jmeter/SCENARIOS_SUMMARY.md](jmeter/SCENARIOS_SUMMARY.md) pour le dÃ©tail des scÃ©narios.

## ğŸ› ï¸ Monitoring & Alerting
- **Prometheus** collecte les mÃ©triques de chaque service (latence, RPS, erreurs)
- **Grafana** propose un dashboard de comparaison
- **Alertes** configurÃ©es sur CPU, mÃ©moire, disponibilitÃ© (voir `monitoring/prometheus/alerts.yml`)

## ğŸ“ RÃ©fÃ©rences utiles
- [variant1/README.md](variant1/README.md) â€“ DÃ©tails Variante A
- [jmeter/SCENARIOS_SUMMARY.md](jmeter/SCENARIOS_SUMMARY.md) â€“ DÃ©tail des scÃ©narios de charge
- [monitoring/grafana/dashboards/benchmark-comparison.json](monitoring/grafana/dashboards/benchmark-comparison.json) â€“ Dashboard Grafana

## ğŸ‘¨â€ğŸ’» Auteur
**Halmaoui Abdellah**  
GitHub: [https://github.com/ABDELLAH1234512345](https://github.com/ABDELLAH1234512345)

---

*Pour toute question ou contribution, ouvrez une issue ou un pull request !*
